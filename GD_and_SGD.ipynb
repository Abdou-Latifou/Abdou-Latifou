{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNTxjOK4lIPldXDuqfFm7D2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdou-Latifou/Abdou-Latifou/blob/main/GD_and_SGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKu8Li4Bldw3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# D√©finir la fonction de co√ªt simple : f(x) = x^2\n",
        "def cost(x):\n",
        "    return x ** 2\n",
        "\n",
        "# Son gradient : f'(x) = 2x\n",
        "def grad(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Param√®tres\n",
        "eta = 0.1           # taux d'apprentissage\n",
        "epochs = 20         # nombre d'it√©rations\n",
        "x0_gd = 10          # point de d√©part pour GD\n",
        "x0_sgd = 10         # point de d√©part pour SGD\n",
        "\n",
        "# Pour GD (utilise toute la fonction √† chaque fois)\n",
        "x_gd = [x0_gd]\n",
        "for i in range(epochs):\n",
        "    x_new = x_gd[-1] - eta * grad(x_gd[-1])\n",
        "    x_gd.append(x_new)\n",
        "\n",
        "# Pour SGD (simulateur bruit√©, on ajoute un bruit al√©atoire au gradient)\n",
        "np.random.seed(42)\n",
        "x_sgd = [x0_sgd]\n",
        "for i in range(epochs):\n",
        "    noise = np.random.normal(0, 0.5)\n",
        "    x_new = x_sgd[-1] - eta * (grad(x_sgd[-1]) + noise)\n",
        "    x_sgd.append(x_new)\n",
        "\n",
        "# Tracer la fonction f(x)\n",
        "x_vals = np.linspace(-12, 12, 400)\n",
        "y_vals = cost(x_vals)\n",
        "\n",
        "# Tracer les trajectoires GD et SGD\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_vals, y_vals, label='f(x) = x¬≤', color='lightgray')\n",
        "plt.plot(x_gd, [cost(x) for x in x_gd], marker='o', label='GD', color='blue')\n",
        "plt.plot(x_sgd, [cost(x) for x in x_sgd], marker='x', label='SGD', color='red')\n",
        "plt.title(\"Comparaison entre Gradient Descent et Stochastic Gradient Descent\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîµ Gradient Descent (GD)\n",
        "En bleu.\n",
        "\n",
        "Suit un chemin r√©gulier et direct vers le minimum de la fonction\n",
        "ùëì\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "=\n",
        "ùë•\n",
        "2\n",
        "f(x)=x\n",
        "2\n",
        " .\n",
        "\n",
        "Chaque mise √† jour est pr√©cise, mais n√©cessite toutes les donn√©es.\n",
        "\n",
        "üî¥ Stochastic Gradient Descent (SGD)\n",
        "En rouge.\n",
        "\n",
        "Chemin instable (bruit√©), car il ajoute de l‚Äôal√©a √† chaque √©tape.\n",
        "\n",
        "Avance en zigzag, mais converge souvent plus vite dans les grandes bases de donn√©es."
      ],
      "metadata": {
        "id": "yFDZh1Lnl5Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6iNJ1Vsxl45P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}